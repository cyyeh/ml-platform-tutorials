{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstrate Using MLflow and TensorBoard with CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import IPython\n",
    "import kerastuner as kt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ai/.pyenv/versions/3.7.9/lib/python3.7/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# set seed for reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing CIFAR-10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up MLflow experiment...\n",
      "MLflow tracking uri: http://mlflow:5000\n",
      "Extracting Cifar10 dataset...\n"
     ]
    }
   ],
   "source": [
    "print(f'Setting up MLflow experiment...')\n",
    "experiment_name = 'cifar10-train'\n",
    "mlflow_tracking_uri = os.getenv('MLFLOW_TRACKING_URI')\n",
    "print(f'MLflow tracking uri: {mlflow_tracking_uri}')\n",
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f'Extracting Cifar10 dataset...')\n",
    "tar = tarfile.open('../data/cifar-10-python.tar.gz')\n",
    "tar.extractall(path='../data')\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training/Testing Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference: https://github.com/tensorflow/tensorflow/blob/9011878d87bdeff932e10e2b2d35570be5ef739e/tensorflow/python/keras/datasets/cifar.py#L26\n",
    "def load_batch(fpath, label_key='labels'):\n",
    "    \"\"\"Internal utility for parsing CIFAR data.\n",
    "    Arguments:\n",
    "      fpath: path the file to parse.\n",
    "      label_key: key for label data in the retrieve\n",
    "          dictionary.\n",
    "    Returns:\n",
    "      A tuple `(data, labels)`.\n",
    "    \"\"\"\n",
    "    with open(fpath, 'rb') as f:\n",
    "        d = pickle.load(f, encoding='bytes')\n",
    "        # decode utf8\n",
    "        d_decoded = {}\n",
    "        for k, v in d.items():\n",
    "            d_decoded[k.decode('utf8')] = v\n",
    "        d = d_decoded\n",
    "    data = d['data']\n",
    "    labels = d[label_key]\n",
    "\n",
    "    data = data.reshape(data.shape[0], 3, 32, 32)\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# reference: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/datasets/cifar10.py#L32\n",
    "def load_data():\n",
    "    dir_name = '../data/cifar-10-batches-py'\n",
    "\n",
    "    # load train data\n",
    "    num_train_samples = 50000\n",
    "\n",
    "    x_train = np.empty((num_train_samples, 3, 32, 32), dtype='uint8')\n",
    "    y_train = np.empty((num_train_samples,), dtype='uint8')\n",
    "\n",
    "    for i in range(1, 6):\n",
    "        fpath = f'{dir_name}/data_batch_{i}'\n",
    "        (x_train[(i - 1) * 10000:i * 10000, :, :, :],\n",
    "         y_train[(i - 1) * 10000:i * 10000]) = load_batch(fpath)\n",
    "\n",
    "    # load test data\n",
    "    fpath = f'{dir_name}/test_batch'\n",
    "    x_test, y_test = load_batch(fpath)\n",
    "\n",
    "    y_train = np.reshape(y_train, (len(y_train), 1))\n",
    "    y_test = np.reshape(y_test, (len(y_test), 1))\n",
    "\n",
    "    if K.image_data_format() == 'channels_last':\n",
    "        x_train = x_train.transpose(0, 2, 3, 1)\n",
    "        x_test = x_test.transpose(0, 2, 3, 1)\n",
    "\n",
    "    x_test = x_test.astype(x_train.dtype)\n",
    "    y_test = y_test.astype(y_train.dtype)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/test images...\n",
      "Training data size: 50000\n",
      "Validation data size: 9900\n",
      "Testing data size: 100\n"
     ]
    }
   ],
   "source": [
    "print(f'Loading train/test images...')\n",
    "(train_images, train_labels), (test_images, test_labels) = load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "# split validation data from testing data\n",
    "TESTING_DATA_SIZE = 100\n",
    "test_images, valid_images  = test_images[:TESTING_DATA_SIZE], test_images[TESTING_DATA_SIZE:]\n",
    "test_labels, valid_labels  = test_labels[:TESTING_DATA_SIZE], test_labels[TESTING_DATA_SIZE:]\n",
    "print(f'Training data size: {len(train_images)}')\n",
    "print(f'Validation data size: {len(valid_images)}')\n",
    "print(f'Testing data size: {len(test_images)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning, Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook would demonstrate using hyperparameter tuning for a simple CNN network model, and you can later compare different experiment results using MLflow and TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "When you build a model for hypertuning, you also define the hyperparameter search space in addition to the model architecture. The model you set up for hypertuning is called a hypermodel.\n",
    "\n",
    "You can define a hypermodel through two approaches:\n",
    "\n",
    "- By using a model builder function\n",
    "- By subclassing the HyperModel class of the Keras Tuner API\n",
    "\n",
    "You can also use two pre-defined HyperModel classes - HyperXception and HyperResNet for computer vision applications.\n",
    "\n",
    "In this tutorial, you use a model builder function to define the image classification model. The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.Flatten())\n",
    "    # Tune the number of units in the first Dense layer\n",
    "    # Choose an optimal value between 32-128\n",
    "    hp_units = hp.Int('units', min_value=32, max_value=128, step=32)\n",
    "    model.add(layers.Dense(units=hp_units, activation='relu'))\n",
    "    model.add(layers.Dense(10))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the tuner and perform hypertuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - \n",
    "`RandomSearch`, `Hyperband`, `BayesianOptimization`, and `Sklearn`.\n",
    "\n",
    "In this tutorial, you use the `Hyperband` tuner.\n",
    "\n",
    "To instantiate the Hyperband tuner, you must specify the hypermodel, the `objective` to optimize and the maximum number of epochs to train (`max_epochs`).\n",
    "\n",
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing $1 + log_{factor}($max_epochs$)$ and rounding it up to the nearest integer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='../logs/',\n",
    "                     project_name='keras_tuner')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the hyperparameter search, define a callback to clear the training outputs at the end of every training step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "    def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the hyperparameter search. The arguments for the search method are the same as those used for `tf.keras.model.fit` in addition to the callback above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 Complete [00h 03m 12s]\n",
      "val_accuracy: 0.7083838582038879\n",
      "\n",
      "Best val_accuracy So Far: 0.7136363387107849\n",
      "Total elapsed time: 00h 27m 26s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
      "layer is 128 and the optimal learning rate for the optimizer\n",
      "is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3,\n",
    "                     directory='../logs/',\n",
    "                     project_name='keras_tuner')\n",
    "\n",
    "log_dir = '../logs/tensorboard/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "tuner.search(train_images,\n",
    "             train_labels,\n",
    "             epochs=10,\n",
    "             validation_data=(valid_images, valid_labels),\n",
    "             callbacks=[ClearTrainingOutput(), tensorboard_callback])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "   2/1563 [..............................] - ETA: 1:29 - loss: 2.3075 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0262s vs `on_train_batch_end` time: 0.0879s). Check your callbacks.\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.4634 - accuracy: 0.4714 - val_loss: 1.1555 - val_accuracy: 0.6100\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.0959 - accuracy: 0.6150 - val_loss: 0.9078 - val_accuracy: 0.6600\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 40s 25ms/step - loss: 0.9362 - accuracy: 0.6707 - val_loss: 0.8019 - val_accuracy: 0.7200\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 39s 25ms/step - loss: 0.8339 - accuracy: 0.7075 - val_loss: 0.7586 - val_accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 36s 23ms/step - loss: 0.7598 - accuracy: 0.7326 - val_loss: 0.7546 - val_accuracy: 0.7200\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 40s 26ms/step - loss: 0.6901 - accuracy: 0.7555 - val_loss: 0.7672 - val_accuracy: 0.7100\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 37s 24ms/step - loss: 0.6348 - accuracy: 0.7767 - val_loss: 0.8199 - val_accuracy: 0.7300\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 0.5824 - accuracy: 0.7934 - val_loss: 0.8045 - val_accuracy: 0.7300\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 35s 23ms/step - loss: 0.5325 - accuracy: 0.8114 - val_loss: 0.9901 - val_accuracy: 0.7200\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 35s 22ms/step - loss: 0.4903 - accuracy: 0.8283 - val_loss: 0.7494 - val_accuracy: 0.7300\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.7494 - accuracy: 0.7300\n",
      "testing loss: 0.7493943572044373\n",
      "testing accuracy: 0.7300000190734863\n"
     ]
    }
   ],
   "source": [
    "mlflow.tensorflow.autolog() # enable autologging for MLflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Build the model with the optimal hyperparameters and train it on the data\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "    \n",
    "    log_dir = '../logs/tensorboard/' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "    model.fit(train_images,\n",
    "              train_labels,\n",
    "              epochs=10,\n",
    "              validation_data=(test_images, test_labels),\n",
    "              callbacks=[tensorboard_callback])\n",
    "    \n",
    "    loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "    print(f'testing loss: {loss}')\n",
    "    print(f'testing accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
